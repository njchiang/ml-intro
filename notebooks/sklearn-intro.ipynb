{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn introduction\n",
    "\n",
    "Scikit-learn is one of the most popular and actively developed machine learning libraries out there. It contains nearly every all \"canonical\" preprocessing and classification techniques. The scikit-learn preprocessing and classifier APIs are identical across all techniques, so models can be quickly swapped out.\n",
    "\n",
    "This notebook is just a brief introduction to some of the core features of scikit-learn, specifically the uniform function calls to validation, preprocessing, model fitting and testing. \n",
    "\n",
    "More recently, the Keras deep learning module has introduced a scikit-learn interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some random data to play with.\n",
    "import numpy as np\n",
    "x = (np.random.rand(100, 10) - .5) * 100\n",
    "b = np.random.rand(10, 1)\n",
    "y = np.dot(x, b) + np.random.randn(100, 1)*20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "As will be discussed later, keeping training and testing sets separate is crucial to valid models and inferences. The scikit-learn APIs make it easy to keep them separate, as well as containing the submodule `model_selection` for good data-splitting hygiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)  # randomly 80/20 split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "When doing your projects or working with real data, you'll learn very quickly that the regressions and classifications from this course don't generally work on their own. (Otherwise everyone would be doing this!). Every type of data (imaging, financial, social media, etc) has its own characteristics and thus call for different preprocessing techniques. You can write your own methods using numpy and scipy, and scikit-learn includes quite a few common ones in sklearn.preprocessing.\n",
    "\n",
    "Here is just an example of some of the preprocessing options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preproc\n",
    "scaler = preproc.StandardScaler()  # will zscore the dataset it is given and save the mean and variance\n",
    "x_train_sc = scaler.fit_transform(x_train)  # zscore training set and save mean\n",
    "x_test_sc = scaler.transform(x_test)  # apply mean and variance to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9984014443252817e-17 1.0\n",
      "0.1466607139294716 1.1202232165082364\n"
     ]
    }
   ],
   "source": [
    "# check the mean and variance\n",
    "print(x_train_sc.mean(), x_train_sc.var())\n",
    "print(x_test_sc.mean(), x_test_sc.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models\n",
    "Scikit-learn has the submodule `linear_model`. This module contains many different linear approaches, including ordinary least squares and regularized regressions. Several common ones are demonstrated below, and will be further explored later. For now, notice that the syntax for training and testing is uniform (you can fine-tune free parameters by updating the model object itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.891325478223941\n",
      "0.8930797538114582\n",
      "0.8921442159424512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "ols = LinearRegression()\n",
    "print(ols.fit(x_train_sc, y_train).score(x_test_sc, y_test))\n",
    "# fit a model on the training data, and see how it does on the testing set\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "print(ridge.fit(x_train_sc, y_train).score(x_test_sc, y_test))\n",
    "\n",
    "lasso = Lasso(alpha=1.0)\n",
    "print(lasso.fit(x_train_sc, y_train).score(x_test_sc, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classification\n",
    "Like linear models, classifiers have a common calling structure. We import a bunch of models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize a whole bunch of different classifiers. Each has its own free parameters that you can tweak (e.g. depth for trees, features for forests, kernels for SVM, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \n",
    "         \"Decision Tree\", \"Random Forest\", \n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),  #k = 3\n",
    "    SVC(kernel=\"linear\", C=0.025),  # linear SVM with C = 0.025\n",
    "    SVC(gamma=2, C=1, kernel=\"rbf\"),  # RBF SVM with C = 1, gamma = 2\n",
    "    DecisionTreeClassifier(max_depth=5),  # depth 5 decision tree\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),  # depth 5 random forest with 10 trees\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some random data to demonstrate classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "\n",
    "rng = np.random.RandomState(2)  # add some noise\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.2, random_state=42)\n",
    "    \n",
    "x_train_sc = scaler.fit_transform(x_train)  # zscore training set and save mean\n",
    "x_test_sc = scaler.transform(x_test)  # apply mean and variance to test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors: 0.95\n",
      "Linear SVM: 0.9\n",
      "RBF SVM: 0.95\n",
      "Decision Tree: 0.9\n",
      "Random Forest: 0.9\n",
      "Naive Bayes: 0.95\n",
      "QDA: 0.95\n"
     ]
    }
   ],
   "source": [
    "# run models in a loop. because each has a .fit() and .score() method, they can just be swapped out\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(x_train_sc, y_train)\n",
    "    score = clf.score(x_test_sc, y_test)\n",
    "    print(name + \": \" + str(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
